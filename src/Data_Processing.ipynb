{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b759bea-375b-4d91-a3b7-9c4ad365df3c",
   "metadata": {},
   "source": [
    "# Data Processing: Mock Data\n",
    "\n",
    "We will use this notebook for modifying the data we are inputing into our model for training. We strongly recommend creating a **virtual environment** before running the following code. Don't forget to install TensorFlow dependencies into your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d2e04-9be0-4c99-9437-dbaba7bfdce6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports\n",
    "\n",
    "Next, let's invoke the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47306f4c-0be3-4619-9575-d4e0296e8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a753549-b20f-4a02-8ea3-7be69da9698c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "This is included in the TensorFlow library. We intend to use the MovieLens ratings dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "75f64cf6-2216-4e03-8d1a-f19a2afd5c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ratings data.\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6478db7d-9111-4fa7-acd5-6746f472652c",
   "metadata": {},
   "source": [
    "Let's take a look at the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "791f869d-3c6f-4d74-bdf3-9ff09175cede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: \n",
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7], dtype=int64),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n",
      "Movie: \n",
      "{'movie_genres': array([4], dtype=int64),\n",
      " 'movie_id': b'1681',\n",
      " 'movie_title': b'You So Crazy (1994)'}\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "    print(\"Rating: \")\n",
    "    pprint.pprint(x)\n",
    "\n",
    "for x in movies.take(1).as_numpy_iterator():\n",
    "    print(\"Movie: \")\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92bb925-c9ab-402a-b2aa-362233733be8",
   "metadata": {},
   "source": [
    "You can modify the limits of the previous for-loops if you would like to see more examples. The next thing to do is to process the data for saving it into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8f919265-fed3-4fce-97bc-f63fd1b19841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"media_id\": x[\"movie_id\"],\n",
    "    \"media_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"user_rating\": x[\"user_rating\"],\n",
    "})\n",
    "\n",
    "media_items = movies.map(lambda x: {\n",
    "    \"media_id\": x[\"movie_id\"],\n",
    "    \"media_title\": x[\"movie_title\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f44a0f-2491-4a9a-9c68-d6213ecbb146",
   "metadata": {},
   "source": [
    "Now that we have some raw data, we will put it in separate files for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4ab56580-605d-4ed6-ae0f-3a84c7f8900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('../RawData/raw_ratings.csv', mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    for rating in ratings.as_numpy_iterator():\n",
    "        csv_writer.writerow([rating[\"media_id\"], rating[\"media_title\"], rating[\"user_id\"], rating[\"user_rating\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "993ea98d-d082-4337-8212-154517829b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../RawData/raw_media_items.csv', mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    for item in media_items.as_numpy_iterator():\n",
    "        csv_writer.writerow([item[\"media_id\"], item[\"media_title\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac0c15d-b4e2-492a-a91e-f21d10562b0c",
   "metadata": {},
   "source": [
    "Now we are going to import some Anime data from this wonderful [GitHub repository](https://github.com/manami-project/anime-offline-database). The objective is to get an array of anime titles and years so we can use for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6b777df6-9213-477c-a6b7-3712462b4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "54425aac-a53a-4817-ae19-c2b2ac4fb6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../RawData/anime-offline-database.json\", encoding='utf-8') as data_file:\n",
    "    data = json.load(data_file)\n",
    "    \n",
    "def reduce(item):\n",
    "    return {\n",
    "        \"media_title\": item[\"title\"].encode('utf-8'),\n",
    "        \"media_year\": str(item[\"animeSeason\"][\"year\"]).encode('utf-8'),\n",
    "    }\n",
    "\n",
    "animes = list(map(reduce, data[\"data\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999268da-141f-4757-ab38-f017e147843f",
   "metadata": {},
   "source": [
    "We will shuffle the data a little bit before we put it into a CSV file. We will also perform some adjustments for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5a80bcee-5416-45e3-b6c1-298693026f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(animes)\n",
    "\n",
    "with open('../RawData/raw_animes.csv', mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    for anime in animes:\n",
    "        csv_writer.writerow([anime[\"media_title\"], anime[\"media_year\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8627d-0214-4207-87d3-ead80dde25b4",
   "metadata": {},
   "source": [
    "Let's take a look at our data and at the same time do some cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e67b707e-ca89-40a0-bdfe-7895e8e3e60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['357', \"One Flew Over the Cuckoo's Nest\", '138', '4.0'],\n",
      " ['709', 'Strictly Ballroom', '92', '2.0'],\n",
      " ['412', 'Very Brady Sequel, A', '301', '4.0'],\n",
      " ['56', 'Pulp Fiction', '60', '4.0'],\n",
      " ['895', 'Scream 2', '197', '3.0'],\n",
      " ['325', 'Crash', '601', '4.0'],\n",
      " ['95', 'Aladdin', '710', '3.0'],\n",
      " ['92', 'True Romance', '833', '2.0'],\n",
      " ['425', 'Bob Roberts', '916', '5.0'],\n",
      " ['271', 'Starship Troopers', '940', '2.0'],\n",
      " ['355', 'Sphere', '611', '1.0'],\n",
      " ['712', 'Tin Men', '707', '3.0'],\n",
      " ['825', 'Arrival, The', '699', '3.0'],\n",
      " ['240', 'Beavis and Butt-head Do America', '16', '4.0'],\n",
      " ['1150', 'Last Dance', '314', '4.0'],\n",
      " ['684', 'In the Line of Fire', '217', '5.0'],\n",
      " ['124', 'Lone Star', '276', '5.0'],\n",
      " ['294', 'Liar Liar', '510', '3.0'],\n",
      " ['265', 'Hunt for Red October, The', '757', '3.0'],\n",
      " ['465', 'Jungle Book, The', '881', '3.0']]\n",
      "Ratings: 100000\n"
     ]
    }
   ],
   "source": [
    "with open('../RawData/raw_ratings.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    ratings_array = []\n",
    "    for row in csv_reader:\n",
    "        media_id = row[0][2:-1]\n",
    "        media_title = row[1][2:-8]\n",
    "        user_id = row[2][2:-1]\n",
    "        user_rating = row[3]\n",
    "        ratings_array.append([media_id, media_title, user_id, user_rating])\n",
    "        \n",
    "pprint.pprint(ratings_array[:20])\n",
    "print(\"Ratings:\", len(ratings_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4abff8b3-6f9f-49bf-807b-ec5e35692530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1681', 'You So Crazy'],\n",
      " ['1457', 'Love Is All There Is'],\n",
      " ['500', 'Fly Away Home'],\n",
      " ['838', 'In the Line of Duty 2'],\n",
      " ['1648', 'Niagara, Niagara'],\n",
      " ['547', \"Young Poisoner's Handbook, The\"],\n",
      " ['387', 'Age of Innocence, The'],\n",
      " ['1495', 'Flirt'],\n",
      " ['817', 'Frisk'],\n",
      " ['267', ''],\n",
      " ['1637', 'Girls Town'],\n",
      " ['1396', 'Stonewall'],\n",
      " ['498', 'African Queen, The'],\n",
      " ['852', 'Bloody Child, The'],\n",
      " ['685', 'Executive Decision'],\n",
      " ['231', 'Batman Returns'],\n",
      " ['719', 'Canadian Bacon'],\n",
      " ['308', 'FairyTale: A True Story'],\n",
      " ['445', 'Body Snatcher, The'],\n",
      " ['486', 'Sabrina']]\n",
      "Movies: 1682\n"
     ]
    }
   ],
   "source": [
    "with open('../RawData/raw_media_items.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    media_items_array = []\n",
    "    for row in csv_reader:\n",
    "        media_id = row[0][2:-1]\n",
    "        media_title = row[1][2:-8]\n",
    "        media_items_array.append([media_id, media_title])\n",
    "        \n",
    "pprint.pprint(media_items_array[:20])\n",
    "print(\"Movies:\", len(media_items_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "247e1a24-5b8e-4f67-a902-315a547ca189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Classroom\\\\xe2\\\\x98\\\\x86Crisis', '2015'],\n",
      " ['Highlander: The Search for Vengeance', '2007'],\n",
      " ['Chuuken Mochi Shiba', 'None'],\n",
      " ['Renou Xueyuan', '2021'],\n",
      " ['Jashin-chan Dropkick Episode 12', '2018'],\n",
      " ['Ketsuekigata-kun! 3', '2015'],\n",
      " ['Sanguo Yanyi 2nd Season: Zhulu Zhongyuan', '2019'],\n",
      " ['Danball Senki', '2011'],\n",
      " ['8-gatsu no Symphony: Shibuya 2002-2003', '2009'],\n",
      " ['Koisuru Boukun', '2010'],\n",
      " ['Chang Jian Fengyun 2', 'None'],\n",
      " ['Mahoutsukai no Yome', '2017'],\n",
      " ['New Big Head Son and Little Head Dad Season 3', '2015'],\n",
      " ['SINBAD', '2016'],\n",
      " ['Message Song', '1996'],\n",
      " ['Oren no yurai', '2003'],\n",
      " ['Yakimochi Caprice', '2011'],\n",
      " ['Kanojo to Kanojo no Neko: Everything Flows', '2016'],\n",
      " ['Garakuta-doori no Stain', '2003'],\n",
      " ['Huanbao Tegong Dui', 'None']]\n",
      "Animes: 32967\n"
     ]
    }
   ],
   "source": [
    "with open('../RawData/raw_animes.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    animes_array = []\n",
    "    for row in csv_reader:\n",
    "        media_title = row[0][2:-1]\n",
    "        media_year = row[1][2:-1]\n",
    "        animes_array.append([media_title, media_year])\n",
    "        \n",
    "pprint.pprint(animes_array[:20])\n",
    "print(\"Animes:\", len(animes_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572b888e-1d22-413d-954c-c156ea7ba6ab",
   "metadata": {},
   "source": [
    "Now we are going to replace half of the movies with anime titles. Since the anime titles are already shuffled, let's do the same with me movies too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "81038579-841d-44fc-96bc-718538b49faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1198', 'Purple Noon'],\n",
      " ['939', 'Murder in the First'],\n",
      " ['1141', 'War Room, The'],\n",
      " ['1601', 'Office Killer'],\n",
      " ['922', 'Dead Man'],\n",
      " ['1190', 'That Old Feeling'],\n",
      " ['230', 'Star Trek IV: The Voyage Home'],\n",
      " ['607', 'Rebecca'],\n",
      " ['1435', 'Steal Big, Steal Little'],\n",
      " ['1557', 'Yankee Zulu'],\n",
      " ['914', 'Wild Things'],\n",
      " ['1398', 'Anna'],\n",
      " ['365', 'Powder'],\n",
      " ['1414', 'Coldblooded'],\n",
      " ['1078', 'Oliver & Company'],\n",
      " ['505', 'Dial M for Murder'],\n",
      " ['794', 'It Could Happen to You'],\n",
      " ['771', 'Johnny Mnemonic'],\n",
      " ['1133', 'Escape to Witch Mountain'],\n",
      " ['1127', 'Truman Show, The']]\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(media_items_array)\n",
    "\n",
    "pprint.pprint(media_items_array[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ca579ab-72a0-4433-9446-a71e3ca738de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1198', 'Classroom\\\\xe2\\\\x98\\\\x86Crisis'],\n",
      " ['939', 'Highlander: The Search for Vengeance'],\n",
      " ['1141', 'Chuuken Mochi Shiba'],\n",
      " ['1601', 'Renou Xueyuan'],\n",
      " ['922', 'Jashin-chan Dropkick Episode 12'],\n",
      " ['1190', 'Ketsuekigata-kun! 3'],\n",
      " ['230', 'Sanguo Yanyi 2nd Season: Zhulu Zhongyuan'],\n",
      " ['607', 'Danball Senki'],\n",
      " ['1435', '8-gatsu no Symphony: Shibuya 2002-2003'],\n",
      " ['1557', 'Koisuru Boukun'],\n",
      " ['914', 'Chang Jian Fengyun 2'],\n",
      " ['1398', 'Mahoutsukai no Yome'],\n",
      " ['365', 'New Big Head Son and Little Head Dad Season 3'],\n",
      " ['1414', 'SINBAD'],\n",
      " ['1078', 'Message Song'],\n",
      " ['505', 'Oren no yurai'],\n",
      " ['794', 'Yakimochi Caprice'],\n",
      " ['771', 'Kanojo to Kanojo no Neko: Everything Flows'],\n",
      " ['1133', 'Garakuta-doori no Stain'],\n",
      " ['1127', 'Huanbao Tegong Dui']]\n",
      "Movies: 1682\n"
     ]
    }
   ],
   "source": [
    "for i in range(int(len(media_items_array)/2)):\n",
    "    media_items_array[i][1] = animes_array[i][0]\n",
    "    \n",
    "pprint.pprint(media_items_array[:20])\n",
    "print(\"Movies:\", len(media_items_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce5a21d-74c3-4657-85c3-c1205a8d87c2",
   "metadata": {},
   "source": [
    "As we can see, our data remains of the same length. Let's write this in a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6d319f7e-18bc-4264-ae71-dfeb79c913d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../RawData/modified_media_items.csv', mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow([\"record_id\", \"media_id\", \"media_title\"])\n",
    "\n",
    "    record_id = 0\n",
    "    for item in media_items_array:\n",
    "        record_id = record_id + 1\n",
    "        csv_writer.writerow([record_id, item[0], item[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef6d0d-4a90-4aa6-83e3-dd5a740f4735",
   "metadata": {},
   "source": [
    "Finally, let's also modify the ratings data by replacing the old values with our updated ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e7216009-3cff-400a-9a5f-d64319fb207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['357', 'Sifan', '138', '4.0'],\n",
      " ['709', 'Strictly Ballroom', '92', '2.0'],\n",
      " ['412', 'Very Brady Sequel, A', '301', '4.0'],\n",
      " ['56', 'Pulp Fiction', '60', '4.0'],\n",
      " ['895', 'Kobo-chan: Matsuri ga Ippai!', '197', '3.0'],\n",
      " ['325', 'Crash', '601', '4.0'],\n",
      " ['95', 'Zhun Xing', '710', '3.0'],\n",
      " ['92', 'True Romance', '833', '2.0'],\n",
      " ['425', 'Bob Roberts', '916', '5.0'],\n",
      " ['271', 'Starship Troopers', '940', '2.0'],\n",
      " ['355', 'Sphere', '611', '1.0'],\n",
      " ['712', 'Baosheng Dadi Zhi Qi Er Duo Bao', '707', '3.0'],\n",
      " ['825', 'Arrival, The', '699', '3.0'],\n",
      " ['240', 'Zhu Zhu Xia: Jing Qiu Xiao Yingxiong', '16', '4.0'],\n",
      " ['1150', 'Last Dance', '314', '4.0'],\n",
      " ['684', 'In the Line of Fire', '217', '5.0'],\n",
      " ['124', 'Lone Star', '276', '5.0'],\n",
      " ['294', 'Karakuri Circus (TV)', '510', '3.0'],\n",
      " ['265', 'Haitoku no Kyoukai', '757', '3.0'],\n",
      " ['465', 'Chan Shuo A Kuan', '881', '3.0']]\n",
      "Ratings: 100000\n"
     ]
    }
   ],
   "source": [
    "for rating in ratings_array:\n",
    "    for item in media_items_array:\n",
    "        if item[0] == rating[0]:\n",
    "            rating[1] = item[1]\n",
    "            \n",
    "pprint.pprint(ratings_array[:20])\n",
    "print(\"Ratings:\", len(ratings_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda88dc-d457-49d5-8b15-7d8e650aa516",
   "metadata": {},
   "source": [
    "Finally, let's save our progress in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cca2ac82-d1ca-4bce-9086-aba33bf0de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../RawData/modified_ratings.csv', mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow([\"record_id\", \"media_id\", \"media_title\", \"user_id\", \"user_rating\"])\n",
    "\n",
    "    record_id = 0\n",
    "    for rating in ratings_array:\n",
    "        record_id = record_id + 1\n",
    "        csv_writer.writerow([record_id, rating[0], rating[1], rating[2], rating[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deff0d9-a6b6-453d-8486-fcf79bba50d8",
   "metadata": {},
   "source": [
    "Since shuffling data modifies the original array, we recommend running the Notebook from the begginning in case you would like to reset the data. Now we have everything we need for creating TensorFlow Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807cd1b-9ac5-43f9-8add-b63938f2f6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
